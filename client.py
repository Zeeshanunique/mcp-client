# Import necessary libraries
import asyncio  # For handling asynchronous operations
import os       # For environment variable access
import sys      # For system-specific parameters and functions
import json     # For handling JSON data (used when printing function declarations)

# Import MCP client components
from typing import Optional, Any, Dict, List  # For type hinting optional values
from contextlib import AsyncExitStack  # For managing multiple async tasks
from mcp import ClientSession, StdioServerParameters  # MCP session management
from mcp.client.stdio import stdio_client  # MCP client for standard I/O communication

# Import Google's Gen AI SDK
from google import genai
from google.genai import types
from google.genai.types import Tool, FunctionDeclaration
from google.genai.types import GenerateContentConfig

from dotenv import load_dotenv  # For loading API keys from a .env file

# Load environment variables from .env file
load_dotenv()

class MCPClient:
    def __init__(self):
        """Initialize the MCP client and configure the Gemini API."""
        self.session: Optional[ClientSession] = None  # MCP session for communication
        self.exit_stack = AsyncExitStack()  # Manages async resource cleanup
        self.conversation_history = []  # Store conversation history

        # Retrieve the Gemini API key from environment variables
        gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not gemini_api_key:
            raise ValueError("GEMINI_API_KEY not found. Please add it to your .env file.")

        # Configure the Gemini AI client
        self.genai_client = genai.Client(api_key=gemini_api_key)

    async def connect_to_server(self, server_script_path: str):
        """Connect to the MCP server and list available tools."""

        # Determine whether the server script is written in Python or JavaScript
        # This allows us to execute the correct command to start the MCP server
        command = "python" if server_script_path.endswith('.py') else "node"

        # Define the parameters for connecting to the MCP server
        server_params = StdioServerParameters(command=command, args=[server_script_path])

        # Establish communication with the MCP server using standard input/output (stdio)
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))

        # Extract the read/write streams from the transport object
        self.stdio, self.write = stdio_transport

        # Initialize the MCP client session, which allows interaction with the server
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))

        # Send an initialization request to the MCP server
        await self.session.initialize()

        # Request the list of available tools from the MCP server
        response = await self.session.list_tools()
        tools = response.tools  # Extract the tool list from the response

        # Check if websearch is present among tools
        has_websearch = any(tool.name == "websearch" for tool in tools)
        
        # Print a message showing the names of the tools available on the server
        print("\nConnected to server with tools:", [tool.name for tool in tools])
        
        # Check SerpAPI configuration if websearch is available
        if has_websearch:
            serpapi_key = os.getenv("SERPAPI_KEY")
            if not serpapi_key:
                print("\n[WARNING] SERPAPI_KEY environment variable not set.")
                print("Web search functionality will not work correctly.")
                print("Please add your SerpAPI key to your .env file: SERPAPI_KEY=your_key_here")
            else:
                print(f"\n[OK] SERPAPI_KEY is configured (length: {len(serpapi_key)})")

        # Convert MCP tools to Gemini format
        self.function_declarations = convert_mcp_tools_to_gemini(tools)

    async def process_query(self, query: str) -> str:
        """
        Process a user query using the Gemini API and execute tool calls if needed.
        
        Args:
            query (str): The user's input query.

        Returns:
            str: The response generated by the Gemini model.
        """
        # Initialize conversation context if this is a new conversation
        if not self.conversation_history:
            system_message = {
                "role": "user",
                "parts": [{"text": "You are a helpful assistant. When you need information, use available tools instead of saying you don't have access to data."}]
            }
            system_response = {
                "role": "model",
                "parts": [{"text": "I'll help you and use available tools when needed instead of saying I don't have access to data."}]
            }
            self.conversation_history = [system_message, system_response]
        
        # Add user query to conversation
        user_message = {
            "role": "user",
            "parts": [{"text": query}]
        }
        self.conversation_history.append(user_message)
        
        # Create a working copy of the conversation history
        messages = self.conversation_history.copy()
        
        # Get all responses and tool calls
        final_text = ""
        
        try:
            # Initial call to Gemini with tools
            response = self.genai_client.models.generate_content(
                model="gemini-1.5-flash",
                contents=messages,
                config=GenerateContentConfig(
                    temperature=0.2, 
                    max_output_tokens=2048,
                    tools=self.function_declarations
                )
            )
            
            # Check if the response contains text or function calls
            if not response.candidates or not response.candidates[0].content:
                return "I couldn't generate a response. Please try again."
            
            # Get the model's response
            candidate = response.candidates[0]
            
            # Check if there are function calls
            has_function_calls = any(
                hasattr(part, "function_call") 
                for part in candidate.content.parts
            )
            
            if not has_function_calls:
                # Simple text response - add to history and return
                text_response = ""
                for part in candidate.content.parts:
                    if hasattr(part, "text") and part.text:
                        text_response += part.text
                
                model_response = {
                    "role": "model",
                    "parts": [{"text": text_response}]
                }
                self.conversation_history.append(model_response)
                return text_response.strip()
            
            # Function calls exist - handle them separately
            # First, add the model's request with function calls to history
            model_parts = []
            function_calls = []
            
            # Get all function calls from the model response
            for part in candidate.content.parts:
                if hasattr(part, "function_call"):
                    function_call = part.function_call
                    function_calls.append({
                        "name": function_call.name,
                        "args": function_call.args
                    })
                    model_parts.append({
                        "function_call": {
                            "name": function_call.name,
                            "args": function_call.args
                        }
                    })
                elif hasattr(part, "text") and part.text:
                    model_parts.append({"text": part.text})
            
            # Add model response with function calls to history
            model_message = {
                "role": "model",
                "parts": model_parts
            }
            self.conversation_history.append(model_message)
            
            # Process all function calls and collect responses
            function_responses = []
            
            for func_call in function_calls:
                tool_name = func_call["name"]
                tool_args = func_call["args"]
                
                print(f"\n[Using tool: {tool_name} with args: {tool_args}]")
                
                try:
                    # Execute the tool
                    result = await self.session.call_tool(tool_name, tool_args)
                    response_content = result.content
                    
                    print(f"\n[Tool response: {response_content}]")
                    
                    # Add to function responses
                    function_responses.append({
                        "name": tool_name,
                        "response": {"result": response_content}
                    })
                    
                except Exception as e:
                    # Handle error in tool execution
                    error_msg = str(e)
                    print(f"\n[Tool error: {error_msg}]")
                    
                    # Add error to function responses
                    function_responses.append({
                        "name": tool_name,
                        "response": {"error": error_msg}
                    })
            
            # Add the function response parts to conversation
            # IMPORTANT: We need exactly one response message with all responses
            function_parts = []
            for resp in function_responses:
                function_parts.append({
                    "function_response": resp
                })
            
            function_message = {
                "role": "function",
                "parts": function_parts
            }
            self.conversation_history.append(function_message)
            
            # Get the follow-up response with tool results
            follow_up_response = self.genai_client.models.generate_content(
                model="gemini-1.5-flash",
                contents=self.conversation_history,
                config=GenerateContentConfig(
                    temperature=0.2,
                    max_output_tokens=2048
                )
            )
            
            # Extract the final response text
            if follow_up_response.candidates and follow_up_response.candidates[0].content:
                final_text = ""
                for part in follow_up_response.candidates[0].content.parts:
                    if hasattr(part, "text") and part.text:
                        final_text += part.text
                
                # Add the final response to history
                final_message = {
                    "role": "model",
                    "parts": [{"text": final_text}]
                }
                self.conversation_history.append(final_message)
            
            return final_text.strip()
            
        except Exception as e:
            print(f"\n[Error in process_query: {str(e)}]")
            # Remove the user message from history on error to prevent history corruption
            if len(self.conversation_history) > 0 and self.conversation_history[-1]["role"] == "user":
                self.conversation_history.pop()
            return f"I encountered an error: {str(e)}"

    async def chat_loop(self):
        """Run an interactive chat session with the user."""
        print("\nMCP Client Started! Type 'quit' to exit.")
        print("Special commands: 'clear' - Clear conversation history, 'history' - Show conversation history")

        while True:
            query = input("\nQuery: ").strip()
            if query.lower() == 'quit':
                break
            elif query.lower() == 'clear':
                self.conversation_history = []
                print("Conversation history cleared.")
                continue
            elif query.lower() == 'history':
                if not self.conversation_history:
                    print("No conversation history yet.")
                else:
                    print("\n----- Conversation History -----")
                    for i, content in enumerate(self.conversation_history):
                        role = content.get('role', 'unknown')
                        parts = content.get('parts', [])
                        text_parts = [p.get('text', '[Function call]') for p in parts if 'text' in p]
                        text = text_parts[0] if text_parts else '[Function call/response]'
                        print(f"{role.upper()}: {text[:100]}{'...' if len(text) > 100 else ''}")
                    print("---------------------------------")
                continue

            # Process the user's query and display the response
            response = await self.process_query(query)
            print("\n" + response)

    async def cleanup(self):
        """Clean up resources before exiting."""
        await self.exit_stack.aclose()

def clean_schema(schema):
    """
    Recursively removes 'title' and 'default' fields from the JSON schema.
    Also ensures that all properties of type OBJECT have at least one property.

    Args:
        schema (dict): The schema dictionary.

    Returns:
        dict: Cleaned schema without 'title' and 'default' fields and valid OBJECT types.
    """
    if not isinstance(schema, dict):
        return schema

    # Remove unnecessary fields
    schema.pop("title", None)
    schema.pop("default", None)

    # Handle OBJECT type with no properties
    if schema.get("type") == "object" and (not schema.get("properties") or len(schema.get("properties", {})) == 0):
        schema["properties"] = {
            "dummy": {
                "type": "string",
                "description": "Dummy parameter to satisfy API requirements"
            }
        }

    # Recursively clean nested properties
    if "properties" in schema and isinstance(schema["properties"], dict):
        for key, prop in schema["properties"].items():
            schema["properties"][key] = clean_schema(prop)

    return schema

def convert_mcp_tools_to_gemini(mcp_tools):
    """
    Converts MCP tool definitions to the correct format for Gemini API function calling.

    Args:
        mcp_tools (list): List of MCP tool objects with 'name', 'description', and 'inputSchema'.

    Returns:
        list: List of Gemini Tool objects with properly formatted function declarations.
    """
    gemini_tools = []

    for tool in mcp_tools:
        # Ensure inputSchema is a valid JSON schema and clean it
        parameters = clean_schema(tool.inputSchema)
        
        # Debug the schema before creating function declaration
        tool_name = tool.name
        print(f"Processing tool: {tool_name}")

        # Wrap in a Tool object
        function_declaration = FunctionDeclaration(
            name=tool_name,
            description=tool.description,
            parameters=parameters
        )
        
        # Wrap in a Tool object
        gemini_tool = Tool(function_declarations=[function_declaration])
        gemini_tools.append(gemini_tool)

    return gemini_tools

async def main():
    """Main function to start the MCP client."""
    if len(sys.argv) < 2:
        print("Usage: python client.py <path_to_server_script>")
        sys.exit(1)

    client = MCPClient()
    try:
        # Connect to the MCP server and start the chat loop
        await client.connect_to_server(sys.argv[1])
        await client.chat_loop()
    finally:
        # Ensure resources are cleaned up
        await client.cleanup()

if __name__ == "__main__":
    # Run the main function within the asyncio event loop
    asyncio.run(main())

